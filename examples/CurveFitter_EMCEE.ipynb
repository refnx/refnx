{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration of MCMC non-linear regression with EMCEE and refnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`refnx` is a package that can be used for non-linear regression (curvefitting). Here I demonstrate how it can be used to analyse Gaussian curve dataset, with Bayesian MCMC sampling of the posterior distributions of the parameters. This is a very robust way of estimating parameter uncertainties. I will also do the analysis with the `emcee` package for comparison\n",
    "\n",
    "The first step is all the imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import emcee\n",
    "import corner\n",
    "from scipy.optimize import leastsq\n",
    "from refnx.analysis import CurveFitter, Parameter, Parameters, Model, Objective, process_chain\n",
    "from refnx.dataset import Data1D\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to load some data in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = Data1D('gauss_data.txt')\n",
    "plt.errorbar(data.x, data.y, yerr=data.y_err, fmt='.k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gauss(x, p, *args):\n",
    "    # p is a Parameters instance. A quick way of getting all the numerical values out\n",
    "    # is making it into array. However, there alternate ways of access:\n",
    "    # e.g. p['bkg'].value or p[0].value.\n",
    "    p0 = np.array(p)\n",
    "    return p0[0] + p0[1] * np.exp(-((x - p0[2]) / p0[3])**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up initial parameter guesses and lower and upper bounds. The last step is to create a `refnx.Parameters` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bkg = Parameter(0.1, 'bkg', vary=True, bounds=(-1, 1))\n",
    "amp = Parameter(20, 'amp', vary=True, bounds=(0, 30))\n",
    "mu = Parameter(0.1, 'mu', vary=True, bounds=(-5, 5))\n",
    "wid = Parameter(0.1, 'wid', vary=True, bounds=(0.001, 2))\n",
    "\n",
    "# to get numerical values out of p0 you have to use np.array(p0), or refer to each Parameter\n",
    "# by using p0['bkg'].value or p0[0].value.\n",
    "p0 = bkg | amp | mu | wid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse with emcee\n",
    "\n",
    "To start with we'll do the analysis with the `emcee` package. Then we'll repeat the analysis with `refnx.analysis.CurveFitter`. \n",
    "\n",
    "The following functions have to be defined for `emcee`. The log-likelihood, the uniform log-prior and the overall log-posterior probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bounds_varying = np.array([[-1, 0, -5, 0.001], [1, 30, 5, 2]]).T\n",
    "\n",
    "def residuals(theta):\n",
    "    resid = (gauss(data.x, theta) - data.y) / data.y_err\n",
    "    return resid\n",
    "    \n",
    "def lnlike(theta):\n",
    "    # log likelihood\n",
    "    return -0.5 * (np.sum(residuals(theta) ** 2))\n",
    "\n",
    "def lnprior(theta):\n",
    "    # uniform prior\n",
    "    if (np.any(theta > bounds_varying[:, 1])\n",
    "            or np.any(theta < bounds_varying[:, 0])):\n",
    "        return -np.inf\n",
    "    return 0\n",
    "\n",
    "def lnpost(theta):\n",
    "    lp = lnprior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + lnlike(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets fit the data with least squares first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = leastsq(residuals, p0, full_output=True)\n",
    "best_fit = result[0]\n",
    "best_errors = np.sqrt(np.diag(result[1]))\n",
    "for mean, std in zip(best_fit, best_errors):\n",
    "    print(\"{:<12g} +/-  {:<10g}\".format(mean, std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the walkers for `emcee`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ndim, nwalkers = 4, 100\n",
    "pos = np.array([np.array(p0) * (1 + 1e-2 * np.random.randn(ndim))\n",
    "                for i in range(nwalkers)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the `emcee` sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lnpost)\n",
    "a = sampler.run_mcmc(pos, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discard 100 burn in steps for each walker and flatten the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chain = sampler.chain[:, 100:, :].reshape(-1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse with CurveFitter\n",
    "\n",
    "Now we're going to do the analysis using a `refnx.analysis.CurveFitter` instance, it should be a lot simpler than the direct approach above. First setup the curvefitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first setup a model\n",
    "model = Model(p0, fitfunc=gauss)\n",
    "\n",
    "# an objective is composed of a model and data\n",
    "objective = Objective(model, data)\n",
    "\n",
    "# a fitter is constructed\n",
    "fitter = CurveFitter(objective, nwalkers=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all do a least-squares fit, to get a starting point for the sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_leastsq = fitter.fit('least_squares')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the MCMC sampling with CurveFitter instead. There are 100 walkers, we do 1000 steps on each walker. We parallelise using 4 threads. After the sampling discard the first 100 steps of each walker and take every 5th step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fitter.sample(1000, pool=4)\n",
    "res_sampling = process_chain(objective, fitter.chain, nburn=100, nthin=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows the posterior distributions for each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = corner.corner(fitter.sampler.flatchain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what about the fits, are they good?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.errorbar(data.x, data.y, yerr=data.y_err, fmt=\".\")\n",
    "\n",
    "saved_state = np.array(p0)\n",
    "# plot a selection of the samples\n",
    "for pars in objective.pgen(500):\n",
    "    # could also use:\n",
    "    # >>> objective.setp(pars)\n",
    "    # then to calculate the model:\n",
    "    # >>> model(data.x)\n",
    "    plt.plot(data.x, objective.generative(pars), color=\"k\", alpha=0.02)\n",
    "\n",
    "plt.plot(data.x, gauss(data.x, p0), color='r', label='sampling')\n",
    "objective.setp(saved_state)\n",
    "\n",
    "# the leastsq fit overlies the sampling\n",
    "# plt.plot(data.x, gauss(data.x, best_fit), color='g', label='leastsq')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following fit parameters are obtained. Lets compare them to the least squares output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Curvefitter.sampling\")\n",
    "print(objective)\n",
    "\n",
    "print(\"\\nleastsq\")\n",
    "print(\"-------\")\n",
    "print(best_fit, '\\n', best_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
